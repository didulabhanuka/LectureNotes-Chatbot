{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae98a7be",
   "metadata": {},
   "source": [
    "Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00d1f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-huggingface chromadb transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75feed0",
   "metadata": {},
   "source": [
    "2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151d9ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import os, getpass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca207c20",
   "metadata": {},
   "source": [
    "3. Authenticate Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db5f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for your Hugging Face API key if not already set\n",
    "if not os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"):\n",
    "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass(\"Enter your Hugging Face API key: \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc8cc0c",
   "metadata": {},
   "source": [
    "4. Load and Split CTSE Lecture Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1348490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your lecture notes PDF file\n",
    "loader = PyPDFLoader(\"../CTSE_Lecture_Notes.pdf\")  # Replace with your file name\n",
    "documents = loader.load()\n",
    "\n",
    "# Split into chunks (important for context-aware retrieval)\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e739d0e",
   "metadata": {},
   "source": [
    "5. Create Embeddings & Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70866c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = \"./chroma_langchain_db\"\n",
    "\n",
    "# Use Sentence Transformer to convert text into vectors\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "if os.path.exists(persist_directory):\n",
    "    # If already exists, load the existing DB\n",
    "    vector_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "\n",
    "else:\n",
    "    # Otherwise, create and save\n",
    "    vector_store = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    vector_store.persist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c9b1e6",
   "metadata": {},
   "source": [
    "6. Set Up Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b64d3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever that returns top 4 relevant chunks\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 8})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1749706",
   "metadata": {},
   "source": [
    "7. Initialize Flan-T5 Base LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c66ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Hugging Face's flan-t5-base model for QA generation\n",
    "model_id = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "# Create a text2text generation pipeline\n",
    "flan_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "llm = HuggingFacePipeline(pipeline=flan_pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78130138",
   "metadata": {},
   "source": [
    "8. Build Retrieval QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128db24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant. Use the provided context to answer the user's question. If you don't know the answer based on the context, say 'I don't know.'\"),\n",
    "    (\"human\", \"Context:\\n{context}\\n\\nQuestion: {question}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661e1227",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815ec157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Conversational Retrieval QA Chain correctly\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": chat_template}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f740e2df",
   "metadata": {},
   "source": [
    "9. Define Chatbot Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b5880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A chatbot function\n",
    "def chatbot(query: str) -> str:\n",
    "    response = qa_chain.run({\"question\": query})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8262e9f",
   "metadata": {},
   "source": [
    "10. Batch Querying â€“ Test Multiple Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c626907f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example questions\n",
    "questions = [\n",
    "    \"What is software engineering?\",\n",
    "    \"Explain the Agile methodology.\",\n",
    "    \"What are the current trends in AI and ML?\",\n",
    "    \"Define DevOps in software development.\",\n",
    "    \"How does LangChain help in building chatbots?\"\n",
    "]\n",
    "\n",
    "# Process all questions at once\n",
    "for q in questions:\n",
    "    print(f\"\\nðŸŸ¡ Question: {q}\")\n",
    "    print(f\"ðŸŸ¢ Answer: {chatbot(q)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
